{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "import utils as ut\n",
    "import numpy as np\n",
    "from random import seed, shuffle\n",
    "SEED = 1122334455\n",
    "seed(SEED) # set the random seed so that the random permutations can be reproduced again\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\"\"\"\n",
    "    The adult dataset can be obtained from: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "    The code will look for the data files (adult.data, adult.test) in the present directory, if they are not found, it will download them from UCI archive.\n",
    "\"\"\"\n",
    "\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\" % fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from UCI Archive...\" % fname)\n",
    "        \n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "    print('')\n",
    "    return\n",
    "\n",
    "def load_user_data(load_data_size=None):\n",
    "\n",
    "    \"\"\"\n",
    "        if load_data_size is set to None (or if no argument is provided), then we load and return the whole data\n",
    "        if it is a number, say 10000, then we will return randomly selected 10K examples\n",
    "    \"\"\"\n",
    "\n",
    "    attrs = ['country', 'socialNbFollowers', 'socialNbFollows', 'socialProductsLiked',\\\n",
    "             'productsListed', 'productsSold', 'productsPassRate', 'productsWished', 'productsBought',\\\n",
    "             'gender', 'hasAnyApp', 'hasAndroidApp', 'hasIosApp', 'hasProfilePicture',\\\n",
    "             'daysSinceLastLogin', 'seniority'] # attributes with integer values -- the rest are categorical\n",
    "    int_attrs = ['socialNbFollowers', 'socialNbFollows', 'socialProductsLiked', 'productsListed',\\\n",
    "                 'productsSold', 'productsPassRate', 'productsWished', 'productsBought',\\\n",
    "                 'daysSinceLastLogin', 'seniority'] # attributes with integer values -- the rest are categorical\n",
    "    sensitive_attrs = ['gender'] # the fairness constraints will be used for this feature\n",
    "    attrs_to_ignore = ['gender'] # sex and race are sensitive feature so we will not use them in classification, we will not consider fnlwght for classification since its computed externally and it highly predictive for the class (for details, see documentation of the adult data)\n",
    "    attrs_for_classification = set(attrs) - set(attrs_to_ignore)\n",
    "\n",
    "    # adult data comes in two different files, one for training and one for testing, however, we will combine data from both the files\n",
    "    data_files = [\"data_final.csv\"]\n",
    "\n",
    "\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    x_control = {}\n",
    "\n",
    "    attrs_to_vals = {} # will store the values for each attribute for all users\n",
    "    for k in attrs:\n",
    "        if k in sensitive_attrs:\n",
    "            x_control[k] = []\n",
    "        elif k in attrs_to_ignore:\n",
    "            pass\n",
    "        else:\n",
    "            attrs_to_vals[k] = []\n",
    "\n",
    "    for f in data_files:\n",
    "        check_data_file(f)\n",
    "\n",
    "        for line in open(f):\n",
    "            line = line.strip()\n",
    "            if line == \"\": continue # skip empty lines\n",
    "            if line.startswith(\"country\"): # ignore header\n",
    "                continue\n",
    "            line = line.split(\",\")\n",
    "            line = line[:-3]\n",
    "            \n",
    "            y.append(1) #fixed class label because there isn't any\n",
    "\n",
    "            for i in range(0,len(line)):\n",
    "                attr_name = attrs[i]\n",
    "                attr_val = line[i]\n",
    "\n",
    "                if attr_name in sensitive_attrs:\n",
    "                    x_control[attr_name].append(attr_val)\n",
    "                elif attr_name in attrs_to_ignore:\n",
    "                    pass\n",
    "                else:\n",
    "                    attrs_to_vals[attr_name].append(attr_val)\n",
    "                    \n",
    "\n",
    "    def convert_attrs_to_ints(d): # discretize the string attributes\n",
    "        for attr_name, attr_vals in d.items():\n",
    "            if attr_name in int_attrs: continue\n",
    "            uniq_vals = sorted(list(set(attr_vals))) # get unique values\n",
    "\n",
    "            # compute integer codes for the unique values\n",
    "            val_dict = {}\n",
    "            for i in range(0,len(uniq_vals)):\n",
    "                val_dict[uniq_vals[i]] = i\n",
    "            # replace the values with their integer encoding\n",
    "            for i in range(0,len(attr_vals)):\n",
    "                attr_vals[i] = val_dict[attr_vals[i]]\n",
    "            d[attr_name] = attr_vals\n",
    "\n",
    "    \n",
    "    # convert the discrete values to their integer representations\n",
    "    convert_attrs_to_ints(x_control)\n",
    "    convert_attrs_to_ints(attrs_to_vals)\n",
    "    \n",
    "\n",
    "    # if the integer vals are not binary, we need to get one-hot encoding for them\n",
    "    for attr_name in attrs_for_classification:\n",
    "        attr_vals = attrs_to_vals[attr_name]\n",
    "        if attr_name in int_attrs or attr_name in ['hasAnyApp', 'hasAndroidApp', 'hasIosApp',\\\n",
    "            'hasProfilePicture']: #MUST catch everything that is int or binary category, otherwise will fuck up\n",
    "            X.append(attr_vals)\n",
    "\n",
    "        else:            \n",
    "            attr_vals, index_dict = ut.get_one_hot_encoding(attr_vals)\n",
    "            for inner_col in attr_vals.T:                \n",
    "                X.append(inner_col) \n",
    "\n",
    "\n",
    "    # convert to numpy arrays for easy handline\n",
    "    #print(X)\n",
    "    X = np.array(X, dtype=float).T\n",
    "    y = np.array(y, dtype = float)\n",
    "    for k, v in x_control.items(): x_control[k] = np.array(v, dtype=float)\n",
    "        \n",
    "    # shuffle the data\n",
    "    perm = list(range(0,len(y))) # shuffle the data before creating each fold\n",
    "    shuffle(perm)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    for k in x_control.keys():\n",
    "        x_control[k] = x_control[k][perm]\n",
    "\n",
    "    # see if we need to subsample the data\n",
    "    if load_data_size is not None:\n",
    "        print(\"Loading only %d examples from the data\" % load_data_size)\n",
    "        X = X[:load_data_size]\n",
    "        y = y[:load_data_size]\n",
    "        for k in x_control.keys():\n",
    "            x_control[k] = x_control[k][:load_data_size]\n",
    "\n",
    "    return X, y, x_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file 'data_final.csv' in the current directory...\n",
      "File found in current directory..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrecords,labels,classes=load_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
